#!/bin/bash
# slx - SLurm eXtended
# A production-ready SLURM project manager
# Usage: slx <command> [options]

set -e

# Tool info
SLX_VERSION="1.0.0"
SLX_NAME="slx"

# XDG directories (with fallbacks)
XDG_CONFIG_HOME="${XDG_CONFIG_HOME:-$HOME/.config}"
XDG_DATA_HOME="${XDG_DATA_HOME:-$HOME/.local/share}"

# slx-specific paths
SLX_CONFIG_DIR="${XDG_CONFIG_HOME}/${SLX_NAME}"
SLX_CONFIG_FILE="${SLX_CONFIG_DIR}/config.env"
SLX_DATA_DIR="${XDG_DATA_HOME}/${SLX_NAME}"

# Check if terminal supports colors
supports_colors() {
    # Check if stdout is a terminal
    [ -t 1 ] || return 1
    # Check if TERM is set and not "dumb"
    [ -n "$TERM" ] && [ "$TERM" != "dumb" ] || return 1
    # Check if NO_COLOR is set (respect NO_COLOR env var)
    [ -z "$NO_COLOR" ] || return 1
    return 0
}

# Colors for output (using $'...' for portability)
if supports_colors; then
    RED=$'\033[0;31m'
    GREEN=$'\033[0;32m'
    YELLOW=$'\033[1;33m'
    BLUE=$'\033[0;34m'
    CYAN=$'\033[0;36m'
    BOLD=$'\033[1m'
    NC=$'\033[0m' # No Color
else
    # No colors if terminal doesn't support them
    RED=""
    GREEN=""
    YELLOW=""
    BLUE=""
    CYAN=""
    BOLD=""
    NC=""
fi

# Default configuration values
DEFAULT_WORKDIR="${HOME}/workdir"
DEFAULT_PARTITION=""
DEFAULT_ACCOUNT=""
DEFAULT_QOS="normal"
DEFAULT_TIME="1440"
DEFAULT_NODES="1"
DEFAULT_NTASKS="1"
DEFAULT_CPUS="4"
DEFAULT_MEM="50000"
DEFAULT_GPUS=""
DEFAULT_EXCLUDE=""
DEFAULT_NODELIST=""

# Runtime configuration (loaded from config.env)
SLX_WORKDIR=""
SLX_PARTITION=""
SLX_ACCOUNT=""
SLX_QOS=""
SLX_TIME=""
SLX_NODES=""
SLX_NTASKS=""
SLX_CPUS=""
SLX_MEM=""
SLX_GPUS=""
SLX_EXCLUDE=""
SLX_NODELIST=""
SLX_LOG_DIR=""

# Load configuration from config.env if it exists
load_config() {
    if [ -f "$SLX_CONFIG_FILE" ]; then
        # Source the config file safely
        set -a
        source "$SLX_CONFIG_FILE"
        set +a
    fi
    
    # Apply loaded values or defaults
    SLX_WORKDIR="${SLX_WORKDIR:-$DEFAULT_WORKDIR}"
    SLX_PARTITION="${SLX_PARTITION:-$DEFAULT_PARTITION}"
    SLX_ACCOUNT="${SLX_ACCOUNT:-$DEFAULT_ACCOUNT}"
    SLX_QOS="${SLX_QOS:-$DEFAULT_QOS}"
    SLX_TIME="${SLX_TIME:-$DEFAULT_TIME}"
    SLX_NODES="${SLX_NODES:-$DEFAULT_NODES}"
    SLX_NTASKS="${SLX_NTASKS:-$DEFAULT_NTASKS}"
    SLX_CPUS="${SLX_CPUS:-$DEFAULT_CPUS}"
    SLX_MEM="${SLX_MEM:-$DEFAULT_MEM}"
    SLX_GPUS="${SLX_GPUS:-$DEFAULT_GPUS}"
    SLX_EXCLUDE="${SLX_EXCLUDE:-$DEFAULT_EXCLUDE}"
    SLX_NODELIST="${SLX_NODELIST:-$DEFAULT_NODELIST}"
    SLX_LOG_DIR="${SLX_LOG_DIR:-${SLX_WORKDIR}/slurm/logs}"
}

# Save configuration to config.env
save_config() {
    mkdir -p "$SLX_CONFIG_DIR"
    cat > "$SLX_CONFIG_FILE" << EOF
# slx configuration file
# Generated on $(date)

# WORKDIR: Base directory for projects (use a large mount if available)
SLX_WORKDIR="${SLX_WORKDIR}"

# Default SLURM job settings
SLX_PARTITION="${SLX_PARTITION}"
SLX_ACCOUNT="${SLX_ACCOUNT}"
SLX_QOS="${SLX_QOS}"
SLX_TIME="${SLX_TIME}"
SLX_NODES="${SLX_NODES}"
SLX_NTASKS="${SLX_NTASKS}"
SLX_CPUS="${SLX_CPUS}"
SLX_MEM="${SLX_MEM}"
SLX_GPUS="${SLX_GPUS}"
SLX_EXCLUDE="${SLX_EXCLUDE}"
SLX_NODELIST="${SLX_NODELIST}"

# Log directory
SLX_LOG_DIR="${SLX_LOG_DIR}"
EOF
    echo -e "${GREEN}Configuration saved to: ${SLX_CONFIG_FILE}${NC}"
}

# Get user input with default
get_input() {
    local prompt="$1"
    local default="$2"
    local var_name="$3"
    
    if [ -n "$default" ]; then
        echo -ne "${CYAN}$prompt${NC} ${YELLOW}[$default]${NC}: "
    else
        echo -ne "${CYAN}$prompt${NC}: "
    fi
    
    read -r input
    
    if [ -z "$input" ] && [ -n "$default" ]; then
        eval "$var_name='$default'"
    else
        eval "$var_name='$input'"
    fi
}

# ============================================
# Template Processing
# ============================================

# Find template directory (checks multiple locations)
find_template_dir() {
    # Check if running from repo
    local script_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    local repo_templates="${script_dir}/../templates"
    if [ -d "$repo_templates" ]; then
        echo "$repo_templates"
        return 0
    fi
    
    # Check installed location
    local installed_templates="${SLX_DATA_DIR}/templates"
    if [ -d "$installed_templates" ]; then
        echo "$installed_templates"
        return 0
    fi
    
    # Fallback: check if templates are in same dir as script
    if [ -d "${script_dir}/templates" ]; then
        echo "${script_dir}/templates"
        return 0
    fi
    
    return 1
}

# Simple template processor (handles {{VAR}} and {{#VAR}}...{{/VAR}})
# Usage: process_template template_file var1=value1 var2=value2 ...
process_template() {
    local template_file="$1"
    shift
    
    if [ ! -f "$template_file" ]; then
        echo -e "${RED}Error: Template file not found: ${template_file}${NC}" >&2
        return 1
    fi
    
    # Build associative array of variables from arguments
    declare -A vars
    for arg in "$@"; do
        if [[ "$arg" =~ ^([^=]+)=(.+)$ ]]; then
            vars["${BASH_REMATCH[1]}"]="${BASH_REMATCH[2]}"
        fi
    done
    
    # Read template content
    local content=$(cat "$template_file")
    
    # Process conditional blocks {{#VAR}}...{{/VAR}}
    # Handle both single-line and multi-line blocks
    for var_name in "${!vars[@]}"; do
        # Pattern to match: {{#VAR}}...{{/VAR}}
        # Use a loop to handle all occurrences
        while true; do
            # Try to find the block pattern
            local pattern_start="{{#${var_name}}}"
            local pattern_end="{{/${var_name}}}"
            local start_pos="${content%%$pattern_start*}"
            
            # If pattern not found, break
            if [ "$start_pos" = "$content" ]; then
                break
            fi
            
            # Extract content after the start pattern
            local after_start="${content#*$pattern_start}"
            local end_pos="${after_start%%$pattern_end*}"
            
            # If end pattern not found, break (malformed template)
            if [ "$end_pos" = "$after_start" ]; then
                break
            fi
            
            # Extract the block content
            local block_content="$end_pos"
            local full_block="${pattern_start}${block_content}${pattern_end}"
            
            # Check if variable is set and non-empty
            if [ -n "${vars[$var_name]}" ]; then
                # Replace variables in block content
                local processed_block="$block_content"
                for vname in "${!vars[@]}"; do
                    processed_block="${processed_block//\{\{${vname}\}\}/${vars[$vname]}}"
                done
                # Replace the full block with processed content
                content="${content//$full_block/$processed_block}"
            else
                # Remove the entire block (including newlines if present)
                content="${content//$full_block/}"
            fi
        done
    done
    
    # Process simple variable substitutions {{VAR}} (not in conditionals)
    for var_name in "${!vars[@]}"; do
        content="${content//\{\{${var_name}\}\}/${vars[$var_name]}}"
    done
    
    echo "$content"
}

# ============================================
# SLURM Query Helpers (best-effort)
# ============================================

# Check if a command is available
has_cmd() {
    command -v "$1" &>/dev/null
}

# Detect which menu tool is available
detect_menu_tool() {
    if has_cmd whiptail; then
        echo "whiptail"
    elif has_cmd dialog; then
        echo "dialog"
    else
        echo "text"
    fi
}

# Query available partitions from SLURM
slurm_query_partitions() {
    if ! has_cmd sinfo; then
        return 1
    fi
    sinfo -h -o "%P" 2>/dev/null | sed 's/\*$//' | sort -u | grep -v '^$'
}

# Query available accounts for current user
slurm_query_accounts() {
    if ! has_cmd sacctmgr; then
        return 1
    fi
    sacctmgr -n -P show assoc where user="$USER" format=account 2>/dev/null | sort -u | grep -v '^$'
}

# Query available QoS for current user
slurm_query_qos() {
    if ! has_cmd sacctmgr; then
        return 1
    fi
    sacctmgr -n -P show assoc where user="$USER" format=qos 2>/dev/null | tr ',' '\n' | sort -u | grep -v '^$'
}

# Query available nodes with state and partition info
# Returns: nodename|state|partition
slurm_query_nodes() {
    if ! has_cmd sinfo; then
        return 1
    fi
    sinfo -N -h -o "%N|%T|%P" 2>/dev/null | sed 's/\*$//' | sort -u | grep -v '^$'
}

# Query just node names (simple list)
slurm_query_node_names() {
    if ! has_cmd sinfo; then
        return 1
    fi
    sinfo -N -h -o "%N" 2>/dev/null | sort -u | grep -v '^$'
}

# ============================================
# Interactive Menu Helpers
# ============================================

# Single-select menu using whiptail/dialog or fallback
# Usage: menu_select_one "title" "prompt" result_var option1 option2 ...
menu_select_one() {
    local title="$1"
    local prompt="$2"
    local result_var="$3"
    shift 3
    local options=("$@")
    
    if [ ${#options[@]} -eq 0 ]; then
        eval "$result_var=''"
        return 1
    fi
    
    local menu_tool=$(detect_menu_tool)
    
    case "$menu_tool" in
        whiptail|dialog)
            local menu_items=()
            local i=1
            for opt in "${options[@]}"; do
                menu_items+=("$opt" "$i")
                ((i++))
            done
            
            local choice
            choice=$($menu_tool --title "$title" --menu "$prompt" 20 60 12 "${menu_items[@]}" 3>&1 1>&2 2>&3)
            local exit_code=$?
            
            if [ $exit_code -eq 0 ] && [ -n "$choice" ]; then
                eval "$result_var='$choice'"
            else
                eval "$result_var=''"
            fi
            ;;
        text)
            echo ""
            echo -e "${CYAN}${title}${NC}"
            echo -e "${prompt}"
            echo ""
            local i=1
            for opt in "${options[@]}"; do
                echo -e "  ${YELLOW}$i)${NC} $opt"
                ((i++))
            done
            echo -e "  ${YELLOW}0)${NC} Skip / Enter manually"
            echo ""
            echo -ne "${CYAN}Select [1-${#options[@]}, 0 to skip]:${NC} "
            read -r selection
            
            if [[ "$selection" =~ ^[0-9]+$ ]] && [ "$selection" -ge 1 ] && [ "$selection" -le ${#options[@]} ]; then
                eval "$result_var='${options[$((selection-1))]}'"
            else
                eval "$result_var=''"
            fi
            ;;
    esac
}

# Multi-select menu using whiptail/dialog or fallback
# Usage: menu_select_many "title" "prompt" result_var option1 option2 ...
# Result is comma-separated list
menu_select_many() {
    local title="$1"
    local prompt="$2"
    local result_var="$3"
    shift 3
    local options=("$@")
    
    if [ ${#options[@]} -eq 0 ]; then
        eval "$result_var=''"
        return 1
    fi
    
    local menu_tool=$(detect_menu_tool)
    
    case "$menu_tool" in
        whiptail|dialog)
            local menu_items=()
            for opt in "${options[@]}"; do
                # Format: tag item status (OFF by default)
                menu_items+=("$opt" "" "OFF")
            done
            
            local choices
            choices=$($menu_tool --title "$title" --checklist "$prompt" 22 70 15 "${menu_items[@]}" 3>&1 1>&2 2>&3)
            local exit_code=$?
            
            if [ $exit_code -eq 0 ] && [ -n "$choices" ]; then
                # Remove quotes and convert spaces to commas
                local cleaned=$(echo "$choices" | tr -d '"' | tr ' ' ',')
                eval "$result_var='$cleaned'"
            else
                eval "$result_var=''"
            fi
            ;;
        text)
            echo ""
            echo -e "${CYAN}${title}${NC}"
            echo -e "${prompt}"
            echo ""
            local i=1
            for opt in "${options[@]}"; do
                echo -e "  ${YELLOW}$i)${NC} $opt"
                ((i++))
            done
            echo ""
            echo -e "${CYAN}Enter selection (comma-separated numbers, ranges like 1-5, or 0 to skip):${NC}"
            echo -ne "Selection: "
            read -r selection
            
            if [ "$selection" = "0" ] || [ -z "$selection" ]; then
                eval "$result_var=''"
            else
                # Parse ranges and individual numbers
                local selected=()
                IFS=',' read -ra parts <<< "$selection"
                for part in "${parts[@]}"; do
                    part=$(echo "$part" | tr -d ' ')
                    if [[ "$part" =~ ^([0-9]+)-([0-9]+)$ ]]; then
                        # Range
                        local start="${BASH_REMATCH[1]}"
                        local end="${BASH_REMATCH[2]}"
                        for ((j=start; j<=end; j++)); do
                            if [ "$j" -ge 1 ] && [ "$j" -le ${#options[@]} ]; then
                                selected+=("${options[$((j-1))]}")
                            fi
                        done
                    elif [[ "$part" =~ ^[0-9]+$ ]]; then
                        if [ "$part" -ge 1 ] && [ "$part" -le ${#options[@]} ]; then
                            selected+=("${options[$((part-1))]}")
                        fi
                    fi
                done
                
                # Join with commas
                local result=""
                for s in "${selected[@]}"; do
                    [ -n "$result" ] && result+=","
                    result+="$s"
                done
                eval "$result_var='$result'"
            fi
            ;;
    esac
}

# Show cluster resource summary
show_cluster_resources() {
    echo -e "${BLUE}Querying cluster resources...${NC}"
    
    local partitions=$(slurm_query_partitions)
    local accounts=$(slurm_query_accounts)
    local qos_list=$(slurm_query_qos)
    local node_count=$(slurm_query_node_names | wc -l)
    
    echo ""
    if [ -n "$partitions" ]; then
        echo -e "  ${GREEN}✓${NC} Partitions: $(echo "$partitions" | wc -l) available"
    else
        echo -e "  ${YELLOW}!${NC} Could not query partitions"
    fi
    
    if [ -n "$accounts" ]; then
        echo -e "  ${GREEN}✓${NC} Accounts: $(echo "$accounts" | wc -l) available for $USER"
    else
        echo -e "  ${YELLOW}!${NC} Could not query accounts"
    fi
    
    if [ -n "$qos_list" ]; then
        echo -e "  ${GREEN}✓${NC} QoS: $(echo "$qos_list" | wc -l) available"
    else
        echo -e "  ${YELLOW}!${NC} Could not query QoS"
    fi
    
    if [ "$node_count" -gt 0 ]; then
        echo -e "  ${GREEN}✓${NC} Nodes: $node_count available"
    else
        echo -e "  ${YELLOW}!${NC} Could not query nodes"
    fi
    echo ""
}

# Interactive partition selection
select_partition() {
    local result_var="$1"
    local default="$2"
    
    local partitions=$(slurm_query_partitions)
    
    if [ -z "$partitions" ]; then
        get_input "Partition (could not query cluster)" "$default" "$result_var"
        return
    fi
    
    local opts=()
    while IFS= read -r line; do
        [ -n "$line" ] && opts+=("$line")
    done <<< "$partitions"
    
    # Add manual entry option
    opts+=("(manual entry)")
    
    local choice=""
    menu_select_one "Select Partition" "Choose a partition for your jobs:" choice "${opts[@]}"
    
    if [ "$choice" = "(manual entry)" ] || [ -z "$choice" ]; then
        get_input "Partition" "$default" "$result_var"
    else
        eval "$result_var='$choice'"
    fi
}

# Interactive account selection
select_account() {
    local result_var="$1"
    local default="$2"
    
    local accounts=$(slurm_query_accounts)
    
    if [ -z "$accounts" ]; then
        get_input "Account (could not query cluster)" "$default" "$result_var"
        return
    fi
    
    local opts=()
    while IFS= read -r line; do
        [ -n "$line" ] && opts+=("$line")
    done <<< "$accounts"
    
    opts+=("(manual entry)")
    
    local choice=""
    menu_select_one "Select Account" "Choose your SLURM account:" choice "${opts[@]}"
    
    if [ "$choice" = "(manual entry)" ] || [ -z "$choice" ]; then
        get_input "Account" "$default" "$result_var"
    else
        eval "$result_var='$choice'"
    fi
}

# Interactive QoS selection
select_qos() {
    local result_var="$1"
    local default="$2"
    
    local qos_list=$(slurm_query_qos)
    
    if [ -z "$qos_list" ]; then
        get_input "QoS (could not query cluster)" "$default" "$result_var"
        return
    fi
    
    local opts=()
    while IFS= read -r line; do
        [ -n "$line" ] && opts+=("$line")
    done <<< "$qos_list"
    
    opts+=("(manual entry)")
    
    local choice=""
    menu_select_one "Select QoS" "Choose Quality of Service level:" choice "${opts[@]}"
    
    if [ "$choice" = "(manual entry)" ] || [ -z "$choice" ]; then
        get_input "QoS" "$default" "$result_var"
    else
        eval "$result_var='$choice'"
    fi
}

# Interactive node selection (multi-select for exclude/nodelist)
select_nodes() {
    local result_var="$1"
    local default="$2"
    local title="$3"
    local prompt="$4"
    
    local nodes=$(slurm_query_nodes)
    
    if [ -z "$nodes" ]; then
        get_input "$title (could not query cluster)" "$default" "$result_var"
        return
    fi
    
    # Parse nodes with state info for display
    local opts=()
    while IFS='|' read -r name state partition; do
        [ -n "$name" ] && opts+=("$name")
    done <<< "$nodes"
    
    # Remove duplicates
    local unique_opts=($(printf '%s\n' "${opts[@]}" | sort -u))
    
    if [ ${#unique_opts[@]} -gt 50 ]; then
        echo -e "${YELLOW}Large cluster detected (${#unique_opts[@]} nodes).${NC}"
        echo -e "Options: ${CYAN}1)${NC} Interactive menu  ${CYAN}2)${NC} Manual entry (SLURM hostlist)"
        echo -ne "Choice [1/2]: "
        read -r method
        
        if [ "$method" = "2" ]; then
            echo -e "${CYAN}Enter nodes (SLURM hostlist format, e.g. node[01-05,08]):${NC}"
            get_input "$title" "$default" "$result_var"
            return
        fi
    fi
    
    unique_opts+=("(manual entry)")
    
    local choice=""
    menu_select_many "$title" "$prompt" choice "${unique_opts[@]}"
    
    if [ "$choice" = "(manual entry)" ] || [ -z "$choice" ]; then
        get_input "$title (comma-separated or SLURM hostlist)" "$default" "$result_var"
    else
        eval "$result_var='$choice'"
    fi
}

# Print usage information
usage() {
    cat << EOF
${BOLD}slx${NC} - SLurm eXtended v${SLX_VERSION}
A production-ready SLURM project manager

${BOLD}USAGE:${NC}
    slx <command> [options]

${BOLD}PROJECT COMMANDS:${NC}
    init                     Initialize slx configuration
    project new              Create a new project with sbatch/run scripts
    project submit <name>    Submit a project's sbatch file
    project list             List all projects

${BOLD}JOB COMMANDS:${NC}
    submit <script>          Submit a SLURM job script
    list [--user USER]       List all running/pending jobs
    running                  List only running jobs
    pending                  List only pending jobs
    kill <job_id>            Cancel a specific job by ID
    killall                  Cancel all jobs for current user
    logs <job_id>            View logs for a specific job
    tail <job_id>            Tail logs for a running job
    info <job_id> [-n]       Show job info (use -n for nodes only)
    status                   Show summary of all user jobs
    history [--days N]       Show job history (default: 1 day)
    find <pattern>           Find jobs by name pattern
    clean                    Clean old log files (interactive)

${BOLD}OTHER:${NC}
    version                  Show version information
    help                     Show this help message

${BOLD}EXAMPLES:${NC}
    slx init
    slx project new
    slx project submit my-project
    slx submit job.sbatch
    slx list
    slx logs 123456
    slx info 123456 --nodes

${BOLD}CONFIGURATION:${NC}
    Config file: ${SLX_CONFIG_FILE}
    Projects:    \${SLX_WORKDIR}/projects/

EOF
}

# Show version
show_version() {
    echo -e "${BOLD}slx${NC} (SLurm eXtended) version ${SLX_VERSION}"
    echo "Config: ${SLX_CONFIG_FILE}"
    echo "Workdir: ${SLX_WORKDIR}"
}

# Initialize configuration
cmd_init() {
    echo -e "${BLUE}========================================${NC}"
    echo -e "${BLUE}slx - SLurm eXtended Setup${NC}"
    echo -e "${BLUE}========================================${NC}"
    echo ""
    
    # Check if config already exists
    if [ -f "$SLX_CONFIG_FILE" ]; then
        echo -e "${YELLOW}Configuration already exists at: ${SLX_CONFIG_FILE}${NC}"
        echo -ne "${YELLOW}Update existing configuration? [y/N]${NC}: "
        read -r update
        if [ "$update" != "y" ] && [ "$update" != "Y" ]; then
            echo -e "${YELLOW}Keeping existing configuration${NC}"
            return 0
        fi
        # Load existing config as defaults
        load_config
    fi
    
    echo -e "${CYAN}Let's configure slx for your cluster.${NC}"
    echo ""
    
    # WORKDIR - try to detect large mounts
    local suggested_workdir="$SLX_WORKDIR"
    if [ -z "$suggested_workdir" ] || [ "$suggested_workdir" = "$DEFAULT_WORKDIR" ]; then
        # Try common large mount locations
        for dir in "/scratch/$USER" "/data/$USER" "/work/$USER" "$HOME/workdir"; do
            if [ -d "$(dirname "$dir")" ]; then
                suggested_workdir="$dir"
                break
            fi
        done
    fi
    
    get_input "WORKDIR (base directory for projects)" "$suggested_workdir" "SLX_WORKDIR"
    SLX_WORKDIR=$(eval echo "$SLX_WORKDIR")  # Expand variables
    
    # Create workdir if it doesn't exist
    if [ ! -d "$SLX_WORKDIR" ]; then
        echo -ne "${YELLOW}Directory does not exist. Create it? [Y/n]${NC}: "
        read -r create_dir
        if [ "$create_dir" != "n" ] && [ "$create_dir" != "N" ]; then
            mkdir -p "$SLX_WORKDIR"
            mkdir -p "$SLX_WORKDIR/projects"
            echo -e "${GREEN}Created: ${SLX_WORKDIR}${NC}"
        fi
    else
        mkdir -p "$SLX_WORKDIR/projects"
    fi
    
    echo ""
    
    # Show cluster resource summary
    show_cluster_resources
    
    echo -e "${CYAN}SLURM job defaults - select from available options:${NC}"
    echo ""
    
    # Cluster-aware selections for partition/account/QoS
    select_partition "SLX_PARTITION" "$SLX_PARTITION"
    select_account "SLX_ACCOUNT" "$SLX_ACCOUNT"
    select_qos "SLX_QOS" "$SLX_QOS"
    
    echo ""
    echo -e "${CYAN}Resource limits:${NC}"
    get_input "Default time limit (minutes)" "$SLX_TIME" "SLX_TIME"
    get_input "Default nodes" "$SLX_NODES" "SLX_NODES"
    get_input "Default tasks" "$SLX_NTASKS" "SLX_NTASKS"
    get_input "Default CPUs per task" "$SLX_CPUS" "SLX_CPUS"
    get_input "Default memory (MB)" "$SLX_MEM" "SLX_MEM"
    get_input "Default GPUs (leave empty for none)" "$SLX_GPUS" "SLX_GPUS"
    
    echo ""
    echo -e "${CYAN}Node preferences:${NC}"
    
    # Multi-select for preferred nodes (nodelist)
    select_nodes "SLX_NODELIST" "$SLX_NODELIST" \
        "Preferred Nodes (NodeList)" \
        "Select nodes to prefer for jobs (optional):"
    
    # Multi-select for excluded nodes
    select_nodes "SLX_EXCLUDE" "$SLX_EXCLUDE" \
        "Exclude Nodes" \
        "Select nodes to exclude from jobs (optional):"
    
    # Set log directory
    SLX_LOG_DIR="${SLX_WORKDIR}/slurm/logs"
    mkdir -p "$SLX_LOG_DIR"
    
    echo ""
    echo -e "${BLUE}Configuration Summary:${NC}"
    echo -e "  WORKDIR:   ${GREEN}${SLX_WORKDIR}${NC}"
    echo -e "  Partition: ${GREEN}${SLX_PARTITION:-<not set>}${NC}"
    echo -e "  Account:   ${GREEN}${SLX_ACCOUNT:-<not set>}${NC}"
    echo -e "  QoS:       ${GREEN}${SLX_QOS:-<not set>}${NC}"
    echo -e "  Time:      ${GREEN}${SLX_TIME} min${NC}"
    echo -e "  Nodes:     ${GREEN}${SLX_NODES}${NC}"
    echo -e "  CPUs:      ${GREEN}${SLX_CPUS}${NC}"
    echo -e "  Memory:    ${GREEN}${SLX_MEM} MB${NC}"
    echo -e "  GPUs:      ${GREEN}${SLX_GPUS:-<none>}${NC}"
    echo -e "  NodeList:  ${GREEN}${SLX_NODELIST:-<none>}${NC}"
    echo -e "  Exclude:   ${GREEN}${SLX_EXCLUDE:-<none>}${NC}"
    echo ""
    
    echo -ne "${YELLOW}Save this configuration? [Y/n]${NC}: "
    read -r save
    if [ "$save" != "n" ] && [ "$save" != "N" ]; then
        save_config
        echo ""
        echo -e "${GREEN}slx is now configured!${NC}"
        echo -e "Create a new project with: ${CYAN}slx project new${NC}"
    else
        echo -e "${YELLOW}Configuration not saved${NC}"
    fi
}

# Project management commands
cmd_project() {
    local subcmd="$1"
    shift || true
    
    case "$subcmd" in
        new)
            project_new "$@"
            ;;
        submit)
            project_submit "$@"
            ;;
        list)
            project_list "$@"
            ;;
        ""|help)
            echo -e "${BOLD}Project commands:${NC}"
            echo "  slx project new              Create a new project"
            echo "  slx project submit <name>    Submit a project's job"
            echo "  slx project list             List all projects"
            ;;
        *)
            echo -e "${RED}Unknown project command: $subcmd${NC}"
            echo "Run 'slx project help' for usage"
            exit 1
            ;;
    esac
}

# Create a new project
project_new() {
    echo -e "${BLUE}========================================${NC}"
    echo -e "${BLUE}Create New Project${NC}"
    echo -e "${BLUE}========================================${NC}"
    echo ""
    
    # Check if initialized
    if [ ! -f "$SLX_CONFIG_FILE" ]; then
        echo -e "${YELLOW}slx is not initialized. Running init first...${NC}"
        echo ""
        cmd_init
        echo ""
    fi
    
    local PROJECT_NAME=""
    local RUN_NAME="run"
    local SBATCH_NAME=""
    local JOB_NAME=""
    
    # Project name (required)
    while [ -z "$PROJECT_NAME" ]; do
        get_input "Project name (required)" "" "PROJECT_NAME"
        if [ -z "$PROJECT_NAME" ]; then
            echo -e "${RED}Project name is required${NC}"
        fi
    done
    
    # Sanitize project name (replace spaces with underscores)
    PROJECT_NAME=$(echo "$PROJECT_NAME" | tr ' ' '_' | tr -cd '[:alnum:]_-')
    
    local PROJECT_DIR="${SLX_WORKDIR}/projects/${PROJECT_NAME}"
    
    # Check if project already exists
    if [ -d "$PROJECT_DIR" ]; then
        echo -e "${YELLOW}Project '${PROJECT_NAME}' already exists at: ${PROJECT_DIR}${NC}"
        echo -ne "${YELLOW}Overwrite? [y/N]${NC}: "
        read -r overwrite
        if [ "$overwrite" != "y" ] && [ "$overwrite" != "Y" ]; then
            echo "Cancelled"
            return 1
        fi
    fi
    
    echo ""
    get_input "Run script name" "$RUN_NAME" "RUN_NAME"
    
    # Default sbatch name is based on run name
    SBATCH_NAME="${RUN_NAME}"
    get_input "Sbatch file name" "$SBATCH_NAME" "SBATCH_NAME"
    
    # Default job name is project name
    JOB_NAME="$PROJECT_NAME"
    get_input "SLURM job name" "$JOB_NAME" "JOB_NAME"
    
    echo ""
    echo -e "${CYAN}Job resource settings (defaults from config, override as needed):${NC}"
    echo ""
    
    local P_PARTITION="$SLX_PARTITION"
    local P_ACCOUNT="$SLX_ACCOUNT"
    local P_QOS="$SLX_QOS"
    local P_TIME="$SLX_TIME"
    local P_NODES="$SLX_NODES"
    local P_NTASKS="$SLX_NTASKS"
    local P_CPUS="$SLX_CPUS"
    local P_MEM="$SLX_MEM"
    local P_GPUS="$SLX_GPUS"
    local P_EXCLUDE="$SLX_EXCLUDE"
    local P_NODELIST="$SLX_NODELIST"
    
    # Check if user wants to customize settings
    echo -e "Current defaults: Partition=${CYAN}${P_PARTITION:-<auto>}${NC}, Account=${CYAN}${P_ACCOUNT:-<auto>}${NC}"
    echo -ne "${YELLOW}Customize job settings? [y/N]${NC}: "
    read -r customize
    
    if [ "$customize" = "y" ] || [ "$customize" = "Y" ]; then
        # Use cluster-aware menus for partition/account/qos
        select_partition "P_PARTITION" "$P_PARTITION"
        select_account "P_ACCOUNT" "$P_ACCOUNT"
        select_qos "P_QOS" "$P_QOS"
        
        echo ""
        echo -e "${CYAN}Resource limits:${NC}"
        get_input "Time (minutes)" "$P_TIME" "P_TIME"
        get_input "Nodes" "$P_NODES" "P_NODES"
        get_input "Tasks" "$P_NTASKS" "P_NTASKS"
        get_input "CPUs per task" "$P_CPUS" "P_CPUS"
        get_input "Memory (MB)" "$P_MEM" "P_MEM"
        get_input "GPUs" "$P_GPUS" "P_GPUS"
        
        echo ""
        echo -e "${CYAN}Node preferences:${NC}"
        select_nodes "P_NODELIST" "$P_NODELIST" \
            "Preferred Nodes (NodeList)" \
            "Select preferred nodes for this project:"
        select_nodes "P_EXCLUDE" "$P_EXCLUDE" \
            "Exclude Nodes" \
            "Select nodes to exclude for this project:"
    fi
    
    echo ""
    echo -e "${BLUE}Project Summary:${NC}"
    echo -e "  Name:      ${GREEN}${PROJECT_NAME}${NC}"
    echo -e "  Directory: ${GREEN}${PROJECT_DIR}${NC}"
    echo -e "  Run:       ${GREEN}${RUN_NAME}.sh${NC}"
    echo -e "  Sbatch:    ${GREEN}${SBATCH_NAME}.sbatch${NC}"
    echo ""
    
    echo -ne "${YELLOW}Create project? [Y/n]${NC}: "
    read -r confirm
    if [ "$confirm" = "n" ] || [ "$confirm" = "N" ]; then
        echo "Cancelled"
        return 1
    fi
    
    # Create project structure
    mkdir -p "$PROJECT_DIR/logs"
    
    # Generate sbatch file from template
    local SBATCH_FILE="${PROJECT_DIR}/${SBATCH_NAME}.sbatch"
    local TEMPLATE_DIR=$(find_template_dir)
    
    if [ -z "$TEMPLATE_DIR" ] || [ ! -f "${TEMPLATE_DIR}/job.sbatch.tmpl" ]; then
        # Fallback to direct generation if template not found
        echo -e "${YELLOW}Warning: Template not found, using direct generation${NC}" >&2
        cat > "$SBATCH_FILE" << EOF
#!/bin/bash
#SBATCH --job-name=${JOB_NAME}
EOF
        [ -n "$P_PARTITION" ] && echo "#SBATCH --partition=${P_PARTITION}" >> "$SBATCH_FILE"
        [ -n "$P_ACCOUNT" ] && echo "#SBATCH --account=${P_ACCOUNT}" >> "$SBATCH_FILE"
        [ -n "$P_QOS" ] && echo "#SBATCH --qos=${P_QOS}" >> "$SBATCH_FILE"
        [ -n "$P_TIME" ] && echo "#SBATCH --time=${P_TIME}" >> "$SBATCH_FILE"
        [ -n "$P_NODES" ] && echo "#SBATCH --nodes=${P_NODES}" >> "$SBATCH_FILE"
        [ -n "$P_NTASKS" ] && echo "#SBATCH --ntasks=${P_NTASKS}" >> "$SBATCH_FILE"
        [ -n "$P_CPUS" ] && echo "#SBATCH --cpus-per-task=${P_CPUS}" >> "$SBATCH_FILE"
        [ -n "$P_MEM" ] && echo "#SBATCH --mem=${P_MEM}" >> "$SBATCH_FILE"
        [ -n "$P_GPUS" ] && echo "#SBATCH --gpus=${P_GPUS}" >> "$SBATCH_FILE"
        [ -n "$P_NODELIST" ] && echo "#SBATCH --nodelist=${P_NODELIST}" >> "$SBATCH_FILE"
        [ -n "$P_EXCLUDE" ] && echo "#SBATCH --exclude=${P_EXCLUDE}" >> "$SBATCH_FILE"
        cat >> "$SBATCH_FILE" << EOF
#SBATCH --output=logs/%x_%j.out
#SBATCH --error=logs/%x_%j.err

# Run the main script
bash ${RUN_NAME}.sh
EOF
    else
        # Use template
        process_template "${TEMPLATE_DIR}/job.sbatch.tmpl" \
            "JOB_NAME=${JOB_NAME}" \
            "PARTITION=${P_PARTITION}" \
            "ACCOUNT=${P_ACCOUNT}" \
            "QOS=${P_QOS}" \
            "TIME=${P_TIME}" \
            "NODES=${P_NODES}" \
            "NTASKS=${P_NTASKS}" \
            "CPUS=${P_CPUS}" \
            "MEM=${P_MEM}" \
            "GPUS=${P_GPUS}" \
            "NODELIST=${P_NODELIST}" \
            "EXCLUDE=${P_EXCLUDE}" \
            "RUN_NAME=${RUN_NAME}" > "$SBATCH_FILE"
    fi
    
    # Generate run script
    local RUN_FILE="${PROJECT_DIR}/${RUN_NAME}.sh"
    cat > "$RUN_FILE" << EOF
#!/bin/bash
# ${PROJECT_NAME} - Main run script
# Generated by slx on $(date)

echo "Starting ${PROJECT_NAME}..."
echo "Job ID: \${SLURM_JOB_ID}"
echo "Node: \$(hostname)"
echo "Working directory: \$(pwd)"
echo ""

# ============================================
# Add your code below
# ============================================

echo "Hello from ${PROJECT_NAME}!"

# Example: Activate a virtual environment
# source /path/to/venv/bin/activate

# Example: Run a Python script
# python main.py

# Example: Run with GPU
# python train.py --gpus=\${SLURM_GPUS}

echo ""
echo "Job completed at: \$(date)"
EOF
    
    chmod +x "$RUN_FILE"
    chmod +x "$SBATCH_FILE"
    
    echo ""
    echo -e "${GREEN}Project created successfully!${NC}"
    echo ""
    echo -e "${BLUE}Project structure:${NC}"
    echo "  ${PROJECT_DIR}/"
    echo "    ├── ${RUN_NAME}.sh      # Your main script (edit this)"
    echo "    ├── ${SBATCH_NAME}.sbatch    # SLURM job file"
    echo "    └── logs/              # Job output logs"
    echo ""
    echo -e "${CYAN}Next steps:${NC}"
    echo "  1. Edit your run script: ${RUN_FILE}"
    echo "  2. Submit the job:       slx project submit ${PROJECT_NAME}"
    echo ""
}

# Submit a project
project_submit() {
    local PROJECT_NAME="$1"
    local RUN_NAME="${2:-run}"
    local SBATCH_NAME="${3:-$RUN_NAME}"
    
    if [ -z "$PROJECT_NAME" ]; then
        echo -e "${RED}Error: Please provide a project name${NC}"
        echo "Usage: slx project submit <project_name> [run_name] [sbatch_name]"
        echo ""
        echo "Available projects:"
        project_list
        exit 1
    fi
    
    local PROJECT_DIR="${SLX_WORKDIR}/projects/${PROJECT_NAME}"
    local SBATCH_FILE="${PROJECT_DIR}/${SBATCH_NAME}.sbatch"
    
    if [ ! -d "$PROJECT_DIR" ]; then
        echo -e "${RED}Error: Project '${PROJECT_NAME}' not found${NC}"
        echo "Available projects:"
        project_list
        exit 1
    fi
    
    if [ ! -f "$SBATCH_FILE" ]; then
        echo -e "${RED}Error: Sbatch file not found: ${SBATCH_FILE}${NC}"
        exit 1
    fi
    
    echo -e "${BLUE}Submitting project: ${PROJECT_NAME}${NC}"
    
    # Change to project directory and submit
    cd "$PROJECT_DIR"
    JOB_OUTPUT=$(sbatch "$SBATCH_FILE" 2>&1)
    
    if [ $? -eq 0 ]; then
        JOB_ID=$(echo "$JOB_OUTPUT" | grep -oP '\d+' | head -1)
        echo -e "${GREEN}Job submitted successfully!${NC}"
        echo -e "Job ID: ${GREEN}${JOB_ID}${NC}"
        echo ""
        echo -e "Logs will be at:"
        echo -e "  ${CYAN}${PROJECT_DIR}/logs/${PROJECT_NAME}_${JOB_ID}.out${NC}"
        echo -e "  ${CYAN}${PROJECT_DIR}/logs/${PROJECT_NAME}_${JOB_ID}.err${NC}"
        echo ""
        echo -e "Monitor with: ${CYAN}slx tail ${JOB_ID}${NC}"
    else
        echo -e "${RED}Failed to submit job${NC}"
        echo "$JOB_OUTPUT"
        exit 1
    fi
}

# List all projects
project_list() {
    local PROJECTS_DIR="${SLX_WORKDIR}/projects"
    
    if [ ! -d "$PROJECTS_DIR" ]; then
        echo -e "${YELLOW}No projects directory found at: ${PROJECTS_DIR}${NC}"
        echo "Run 'slx init' first, then 'slx project new' to create a project"
        return 0
    fi
    
    local projects=$(ls -1 "$PROJECTS_DIR" 2>/dev/null)
    
    if [ -z "$projects" ]; then
        echo -e "${YELLOW}No projects found${NC}"
        echo "Create one with: slx project new"
        return 0
    fi
    
    echo -e "${BLUE}Projects in ${PROJECTS_DIR}:${NC}"
    echo ""
    
    for proj in $projects; do
        if [ -d "${PROJECTS_DIR}/${proj}" ]; then
            local sbatch_count=$(ls -1 "${PROJECTS_DIR}/${proj}"/*.sbatch 2>/dev/null | wc -l)
            echo -e "  ${GREEN}${proj}${NC} (${sbatch_count} sbatch file(s))"
        fi
    done
    echo ""
}

# ============================================
# Job operation commands (from cluster.sh)
# ============================================

# Submit a job
submit_job() {
    if [ -z "$1" ]; then
        echo -e "${RED}Error: Please provide a job script to submit${NC}"
        echo "Usage: slx submit <script>"
        exit 1
    fi
    
    if [ ! -f "$1" ]; then
        echo -e "${RED}Error: Script file '$1' not found${NC}"
        exit 1
    fi
    
    echo -e "${BLUE}Submitting job script: $1${NC}"
    JOB_ID=$(sbatch "$1" | grep -oP '\d+')
    
    if [ $? -eq 0 ] && [ -n "$JOB_ID" ]; then
        echo -e "${GREEN}Job submitted successfully!${NC}"
        echo -e "Job ID: ${GREEN}$JOB_ID${NC}"
        echo "View logs with: slx logs $JOB_ID"
        echo "Monitor with: slx tail $JOB_ID"
    else
        echo -e "${RED}Failed to submit job${NC}"
        exit 1
    fi
}

# List jobs
list_jobs() {
    local USER_ARG=""
    if [ "$1" == "--user" ] && [ -n "$2" ]; then
        USER_ARG="-u $2"
    elif [ -z "$USER_ARG" ]; then
        USER_ARG="-u $USER"
    fi
    
    echo -e "${BLUE}Current jobs:${NC}"
    squeue $USER_ARG -o "%.18i %.9P %.20j %.8u %.2t %.10M %.6D %R"
}

# List running jobs only
list_running() {
    echo -e "${BLUE}Running jobs:${NC}"
    squeue -u $USER -t RUNNING -o "%.18i %.9P %.20j %.8u %.2t %.10M %.6D %R"
}

# List pending jobs only
list_pending() {
    echo -e "${BLUE}Pending jobs:${NC}"
    squeue -u $USER -t PENDING -o "%.18i %.9P %.20j %.8u %.2t %.10M %.6D %R"
}

# Kill a specific job
kill_job() {
    if [ -z "$1" ]; then
        echo -e "${RED}Error: Please provide a job ID${NC}"
        echo "Usage: slx kill <job_id>"
        exit 1
    fi
    
    echo -e "${YELLOW}Cancelling job $1...${NC}"
    if scancel "$1"; then
        echo -e "${GREEN}Job $1 cancelled successfully${NC}"
    else
        echo -e "${RED}Failed to cancel job $1${NC}"
        exit 1
    fi
}

# Kill all user jobs
killall_jobs() {
    echo -e "${YELLOW}This will cancel ALL your jobs. Are you sure? (yes/no)${NC}"
    read -r response
    if [ "$response" = "yes" ]; then
        JOB_COUNT=$(squeue -u $USER -h | wc -l)
        if [ "$JOB_COUNT" -eq 0 ]; then
            echo -e "${BLUE}No jobs to cancel${NC}"
        else
            echo -e "${YELLOW}Cancelling $JOB_COUNT job(s)...${NC}"
            scancel -u $USER
            echo -e "${GREEN}All jobs cancelled${NC}"
        fi
    else
        echo "Cancelled"
    fi
}

# View job logs
view_logs() {
    if [ -z "$1" ]; then
        echo -e "${RED}Error: Please provide a job ID${NC}"
        echo "Usage: slx logs <job_id>"
        exit 1
    fi
    
    JOB_ID=$1
    LOG_FILES=()
    
    # Check in various log locations
    # Project logs
    for proj_dir in "${SLX_WORKDIR}/projects"/*; do
        if [ -d "$proj_dir/logs" ]; then
            for f in "$proj_dir/logs"/*_${JOB_ID}.out "$proj_dir/logs"/*_${JOB_ID}.err; do
                [ -f "$f" ] && LOG_FILES+=("$f")
            done
        fi
    done
    
    # Check SLX log dir
    if [ -d "$SLX_LOG_DIR" ]; then
        for f in "$SLX_LOG_DIR"/*${JOB_ID}*.out "$SLX_LOG_DIR"/*${JOB_ID}*.err \
                 "$SLX_LOG_DIR"/*${JOB_ID}*.log; do
            [ -f "$f" ] && LOG_FILES+=("$f")
        done
    fi
    
    # Check current directory
    for f in slurm-${JOB_ID}.out *_${JOB_ID}.out *_${JOB_ID}.err; do
        [ -f "$f" ] && LOG_FILES+=("$f")
    done
    
    if [ ${#LOG_FILES[@]} -eq 0 ]; then
        echo -e "${YELLOW}No log files found for job $JOB_ID${NC}"
        echo "Searched in project logs, ${SLX_LOG_DIR}, and current directory"
        exit 1
    fi
    
    for log_file in "${LOG_FILES[@]}"; do
        echo -e "\n${BLUE}=== $(basename $log_file) ===${NC}"
        cat "$log_file"
    done
}

# Tail job logs
tail_logs() {
    if [ -z "$1" ]; then
        echo -e "${RED}Error: Please provide a job ID${NC}"
        echo "Usage: slx tail <job_id>"
        exit 1
    fi
    
    JOB_ID=$1
    LOG_FILES=()
    
    # Check in various log locations
    for proj_dir in "${SLX_WORKDIR}/projects"/*; do
        if [ -d "$proj_dir/logs" ]; then
            for f in "$proj_dir/logs"/*_${JOB_ID}.out "$proj_dir/logs"/*_${JOB_ID}.err; do
                [ -f "$f" ] && LOG_FILES+=("$f")
            done
        fi
    done
    
    if [ -d "$SLX_LOG_DIR" ]; then
        for f in "$SLX_LOG_DIR"/*${JOB_ID}*.out "$SLX_LOG_DIR"/*${JOB_ID}*.err; do
            [ -f "$f" ] && LOG_FILES+=("$f")
        done
    fi
    
    for f in slurm-${JOB_ID}.out *_${JOB_ID}.out *_${JOB_ID}.err; do
        [ -f "$f" ] && LOG_FILES+=("$f")
    done
    
    if [ ${#LOG_FILES[@]} -eq 0 ]; then
        echo -e "${YELLOW}No log files found for job $JOB_ID${NC}"
        exit 1
    fi
    
    echo -e "${BLUE}Tailing logs for job $JOB_ID (Ctrl+C to exit)${NC}"
    tail -f "${LOG_FILES[@]}"
}

# Show job information
show_info() {
    local JOB_ID=""
    local SHOW_NODES_ONLY=false
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --nodes|-n)
                SHOW_NODES_ONLY=true
                shift
                ;;
            *)
                if [ -z "$JOB_ID" ]; then
                    JOB_ID="$1"
                else
                    echo -e "${RED}Error: Unexpected argument: $1${NC}"
                    echo "Usage: slx info <job_id> [--nodes|-n]"
                    exit 1
                fi
                shift
                ;;
        esac
    done
    
    if [ -z "$JOB_ID" ]; then
        echo -e "${RED}Error: Please provide a job ID${NC}"
        echo "Usage: slx info <job_id> [--nodes|-n]"
        exit 1
    fi
    
    if [ "$SHOW_NODES_ONLY" = true ]; then
        local NODE_INFO=$(scontrol show job "$JOB_ID" 2>/dev/null | grep -E "^[[:space:]]*NodeList=")
        if [ -z "$NODE_INFO" ]; then
            echo -e "${YELLOW}No node information found for job $JOB_ID${NC}"
            echo "Job may not be running or may not have been allocated nodes yet."
            exit 1
        fi
        
        local NODES=$(echo "$NODE_INFO" | sed -n 's/^[[:space:]]*NodeList=\([^ ]*\).*/\1/p')
        
        if [ -z "$NODES" ]; then
            echo -e "${YELLOW}No nodes allocated for job $JOB_ID${NC}"
            exit 1
        fi
        
        echo -e "${BLUE}Nodes for job $JOB_ID:${NC}"
        echo "$NODES" | tr ',' '\n' | sed 's/^/  /'
    else
        echo -e "${BLUE}Job Information:${NC}"
        scontrol show job "$JOB_ID"
    fi
}

# Show job status summary
show_status() {
    echo -e "${BLUE}=== Job Status Summary ===${NC}\n"
    
    RUNNING=$(squeue -u $USER -t RUNNING -h | wc -l)
    PENDING=$(squeue -u $USER -t PENDING -h | wc -l)
    TOTAL=$((RUNNING + PENDING))
    
    echo -e "Total jobs: ${GREEN}$TOTAL${NC}"
    echo -e "  Running: ${GREEN}$RUNNING${NC}"
    echo -e "  Pending: ${YELLOW}$PENDING${NC}"
    
    if [ "$TOTAL" -gt 0 ]; then
        echo ""
        list_jobs
    fi
}

# Show job history
show_history() {
    local DAYS=1
    if [ "$1" == "--days" ] && [ -n "$2" ]; then
        DAYS=$2
    fi
    
    echo -e "${BLUE}Job history (last $DAYS day(s)):${NC}"
    sacct -u $USER --starttime=$(date -d "$DAYS days ago" +%Y-%m-%d) \
          --format=JobID,JobName,Partition,State,ExitCode,Start,End,Elapsed,MaxRSS,ReqMem
}

# Find jobs by name pattern
find_jobs() {
    if [ -z "$1" ]; then
        echo -e "${RED}Error: Please provide a search pattern${NC}"
        echo "Usage: slx find <pattern>"
        exit 1
    fi
    
    echo -e "${BLUE}Jobs matching pattern '$1':${NC}"
    squeue -u $USER -o "%.18i %.9P %.20j %.8u %.2t %.10M %.6D %R" | grep -i "$1"
}

# Clean old log files
clean_logs() {
    echo -e "${YELLOW}This will help you clean old log files interactively.${NC}"
    echo "Enter the number of days to keep (logs older than this will be shown for deletion):"
    read -r days
    
    if ! [[ "$days" =~ ^[0-9]+$ ]]; then
        echo -e "${RED}Invalid number${NC}"
        exit 1
    fi
    
    echo -e "${BLUE}Finding log files older than $days days...${NC}"
    
    # Search in project logs and SLX log dir
    local SEARCH_DIRS=("${SLX_WORKDIR}/projects" "$SLX_LOG_DIR")
    local OLD_FILES=""
    
    for dir in "${SEARCH_DIRS[@]}"; do
        if [ -d "$dir" ]; then
            OLD_FILES+=$(find "$dir" -type f \( -name "*.log" -o -name "*.out" -o -name "*.err" \) \
                        -mtime +$days 2>/dev/null)
            OLD_FILES+=$'\n'
        fi
    done
    
    OLD_FILES=$(echo "$OLD_FILES" | grep -v '^$' | head -20)
    
    if [ -z "$OLD_FILES" ]; then
        echo -e "${GREEN}No old log files found${NC}"
        exit 0
    fi
    
    echo -e "${YELLOW}Found old log files:${NC}"
    echo "$OLD_FILES" | nl
    echo ""
    echo "Delete these files? (yes/no)"
    read -r response
    
    if [ "$response" = "yes" ]; then
        echo "$OLD_FILES" | xargs rm -f
        echo -e "${GREEN}Deleted old log files${NC}"
    else
        echo "Cancelled"
    fi
}

# ============================================
# Main command dispatcher
# ============================================

main() {
    # Load configuration
    load_config
    
    case "$1" in
        init)
            cmd_init
            ;;
        project)
            shift
            cmd_project "$@"
            ;;
        submit)
            shift
            submit_job "$@"
            ;;
        list)
            shift
            list_jobs "$@"
            ;;
        running)
            list_running
            ;;
        pending)
            list_pending
            ;;
        kill)
            shift
            kill_job "$@"
            ;;
        killall)
            killall_jobs
            ;;
        logs)
            shift
            view_logs "$@"
            ;;
        tail)
            shift
            tail_logs "$@"
            ;;
        info)
            shift
            show_info "$@"
            ;;
        status)
            show_status
            ;;
        history)
            shift
            show_history "$@"
            ;;
        find)
            shift
            find_jobs "$@"
            ;;
        clean)
            clean_logs
            ;;
        version|--version|-v)
            show_version
            ;;
        help|--help|-h)
            usage
            ;;
        "")
            usage
            ;;
        *)
            echo -e "${RED}Unknown command: $1${NC}"
            echo ""
            usage
            exit 1
            ;;
    esac
}

# Run main function
main "$@"

